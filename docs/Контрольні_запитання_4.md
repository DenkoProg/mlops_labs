# Контрольні запитання - Відповіді (Лабораторна робота №4)

## 1. У чому різниця між CI та CD у контексті ML?

**CI (Continuous Integration)** — автоматична перевірка кожного коміту: лінтинг, тести даних, тренування, Quality Gate. Мета: виявляти проблеми якомога раніше.

**CD (Continuous Delivery/Deployment)** — автоматична підготовка та публікація артефактів після успішного CI:

| | CI | CD |
|---|---|---|
| **Тригер** | Кожен push/PR | Лише після успішного CI на `main` |
| **Результат** | Звіт про якість | Готовий артефакт (model.joblib, registry) |
| **Приклад (ЛР4)** | pytest + cml comment | upload-artifact на GitHub |

---

## 2. Опишіть структуру GitHub Actions Workflow

```
Workflow (.github/workflows/cml.yaml)
└── Triggered by: push / pull_request    ← Event
    └── Job: test-and-report             ← Job (runs on ubuntu-latest Runner)
        ├── Step: Checkout               ← Step
        ├── Step: Setup uv
        ├── Step: Lint with Ruff
        ├── Step: DVC pull
        ├── Step: Pre-train tests
        ├── Step: dvc repro
        ├── Step: Post-train tests
        └── Step: CML report
```

- **Workflow** — YAML файл, описує весь процес
- **Event** — подія запуску (`push`, `pull_request`)
- **Job** — набір кроків на одному runner (кожен job — чистий контейнер)
- **Runner** — VM/контейнер (у нас `ubuntu-latest`)
- **Step** — окрема команда або `uses:` action

---

## 3. Що таке CML і яку проблему він вирішує?

**CML (Continuous Machine Learning)** — CLI-інструмент від Iterative.ai для інтеграції ML-результатів у Git workflow.

**Проблема без CML:** після тренування в CI результати губляться в логах. Ревʼювер не може порівняти метрики без власного запуску.

**З CML:**
```bash
cml publish confusion_matrix.png --md >> report.md
cml comment create report.md   # публікує як коментар у PR
```

Ревʼювер бачить таблицю метрик і графіки прямо у PR — без запуску коду.

---

## 4. Чому важливо розділяти pre-train та post-train тести?

**Pre-train тести** (швидко, без тренування) дають **ранній зворотний зв'язок**:
- Якщо дані некоректні → не витрачати 10 хвилин на тренування
- Перевіряють: форму масивів, діапазони значень, баланс класів, NaN

**Post-train тести** перевіряють результат:
- Чи збережені артефакти (`metrics.json`, `*.joblib`, `confusion_matrix.png`)
- **Quality Gate**: `test_f1 >= 0.80` — формальний критерій допуску моделі

```
pre-train → (fail fast ←) train → post-train → CML report
```

Такий порядок економить час і дає чіткий сигнал про причину збою.

---

## 5. Що таке Quality Gate і чому він важливий?

**Quality Gate** — формалізований поріг, що автоматично вирішує: чи відповідає модель мінімальним вимогам якості.

У нашому проекті (`tests/test_artifacts.py`):
```python
F1_THRESHOLD = 0.80       # test_f1 >= 0.80
ACCURACY_THRESHOLD = 0.75  # test_accuracy >= 0.75
MAX_OVERFITTING_GAP = 0.15 # train_f1 - test_f1 <= 0.15
```

**Навіщо:** якщо хтось змінить `max_depth=1` або сломає preprocessing — PR автоматично відхиляється, а не йде в production. Це замінює ручну перевірку метрик кожного PR.

---

## 6. Як вирішити проблему доступу до даних у CI/CD?

Головна проблема: `data/raw/chest_xray` (~1.2GB) не зберігається в Git, а CI-runner чиста VM без локального DVC-кешу.

**Варіанти:**

| Варіант | Плюси | Мінуси |
|---|---|---|
| **DVC + S3/GCS remote** | Правильний підхід, дані версіоновані | Потрібен хмарний акаунт + secrets |
| **Commit data до Git** | Просто | Порушує DVC-принципи, ліміти GitHub |
| **Кешувати підготовлені дані** | Швидко в CI (лише `data/prepared/`) | Все ще потрібне сховище |

**Наш підхід (ЛР4):** DVC remote на S3, облікові дані через GitHub Secrets:
```yaml
env:
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
```
```bash
uv run dvc pull   # завантажує data/prepared/ та data/raw/
```

---

## 7. Які переваги використання `uv` у CI порівняно з `pip + requirements.txt`?

| | `pip install -r requirements.txt` | `uv sync --frozen` |
|---|---|---|
| **Швидкість** | ~2-3 хв (compile + resolve) | ~10-30 сек (бінарний інстолятор) |
| **Відтворюваність** | requirements.txt може мати `>=` версії | `uv.lock` — точні версії всіх транзитивних залежностей |
| **Кешування** | `cache: pip` на основі requirements.txt | `enable-cache: true` кешує весь .venv |
| **Файл** | Потрібен окремий requirements.txt | Використовує вже наявний `pyproject.toml` + `uv.lock` |

```yaml
- uses: astral-sh/setup-uv@v5
  with:
    enable-cache: true        # кешує .venv між runs
- run: uv sync --frozen       # встановлює точно як у lockfile
- run: uv run pytest tests/   # виконує в тому ж venv
```

---

## 8. Як CI/CD забезпечує відтворюваність ML-експериментів?

Кожен CI-run фіксує і перевіряє:

1. **Код** → `git commit hash` (автоматично у кожному run)
2. **Дані** → `dvc.lock` містить MD5-хеш даних, `dvc pull` відтворює точну версію
3. **Конфігурація** → `conf/config.yaml` (Hydra) у репозиторії
4. **Залежності** → `uv.lock` з точними версіями бібліотек
5. **Seed** → фіксований у `conf/config.yaml` (`random_state: 42`)

**Результат:** будь-хто може взяти будь-який коміт і відтворити той самий результат командою:
```bash
git checkout <commit>
uv sync --frozen
dvc pull
dvc repro
```

---

## 9. Чому в CML-звіті зображення краще додавати через `cml publish ... --md`, а не як локальне посилання?

**Проблема локального посилання:** GitHub Actions runner є тимчасовою VM. Після завершення job уся файлова система знищується. Посилання виду `![](./confusion_matrix.png)` у PR-коментарі буде посиланням на неіснуючий файл — зображення не відобразиться.

**`cml publish` вирішує це:**
```bash
cml publish confusion_matrix.png --md >> report.md
# Генерує: ![](https://..../confusion_matrix.png)
```

CML завантажує зображення у тимчасове сховище (прив'язане до GitHub/GitLab) і повертає HTTP-URL — він залишається валідним після завершення runner. Ревʼюер бачить графіки прямо у PR без хоститингу файлів.

---

## 10. Як забезпечити доступність даних у CI-раннері, якщо дані не зберігаються в Git?

**Рекомендований підхід: DVC + хмарний remote**

```yaml
# .github/workflows/cml.yaml
- name: DVC pull
  env:
    AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
    AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  run: uv run dvc pull
```

| Хмарний провайдер | Тип секрету у GitHub |
|---|---|
| AWS S3 | `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` |
| Google Cloud Storage | `GOOGLE_APPLICATION_CREDENTIALS` (JSON) |
| Azure Blob | `AZURE_STORAGE_CONNECTION_STRING` |
| DVC Studio | `DVC_STUDIO_TOKEN` |

**Альтернативи для малих датасетів:**
- Git LFS (для файлів до ~100 MB)
- Зберігати лише `data/prepared/` (після preprocessing) та кешувати через `actions/cache`

---

## 11. Які ризики «хибно зеленого» та «хибно червоного» Quality Gate? Як мінімізувати?

**Хибно зелений (False Positive)** — тест проходить, але модель насправді деградувала:
- Причина: поріг занизький, фіксовані тестові дані застаріли, метрика на навчальній вибірці замість тестової.
- Ризик: дефектна модель потрапляє в production.

**Хибно червоний (False Negative)** — тест падає, але зміна насправді коректна:
- Причина: занадто суворий поріг, нестабільний split (без `random_state`), малий тестовий набір.
- Ризик: блокування валідних PR, втрата часу команди.

**Мінімізація:**
1. Фіксувати `random_state` у `train_test_split` та seed моделі.
2. Вибирати поріг на основі статистики з ~10 незалежних запусків (mean - 2·std).
3. Додавати допуск: `f1 >= threshold - 0.005` замість строгої рівності.
4. Версіонувати тестовий набір разом із кодом через DVC.
5. Налаштувати `max_overfitting_gap` для виявлення перенавченості:
```python
assert train_f1 - test_f1 <= 0.15, "Overfitting detected"
```

---

## 12. Запропонуйте підхід до обґрунтування порогу для Quality Gate

**Процес вибору порогу:**

1. **Зібрати статистику базових запусків** — запустити тренування без змін `N=10` разів з різними seeds, записати `test_f1` кожного разу.
2. **Обчислити статистики**: `mean`, `std`, `min`, `median`.
3. **Встановити поріг** = `mean - 2·std` (або `P10` перцентиль) — це дозволяє нормальній варіативності не блокувати PR.
4. **Переглядати поріг** після кожного значного покращення моделі (HPO, нові дані).

**Приклад для нашого проекту:**
```
10 baseline runs: mean=0.882, std=0.003
Threshold = 0.882 - 2*0.003 = 0.876
→ F1_THRESHOLD=0.876 у .env / GitHub Secrets
```

Таким чином поріг заснований на **реальних даних**, а не «магічних числах».

---

## 13. Опишіть дизайн baseline comparison у PR: зберігання, порівняння, відображення у CML-звіті

**Схема:**

1. **Зберігати** `baseline/metrics.json` у гілці `main` із метриками «еталонного» запуску.
2. **У PR workflow** після тренування порівняти Python-скриптом:
```python
import json

with open("baseline/metrics.json") as f:
    baseline = json.load(f)
with open("metrics.json") as f:
    current = json.load(f)

delta_f1 = current["f1"] - baseline["f1"]
delta_acc = current["accuracy"] - baseline["accuracy"]
```

3. **Додати таблицю до CML-звіту:**
```bash
cat >> report.md << EOF
## Baseline Comparison
| Metric | Baseline | PR Branch | Δ |
|--------|----------|-----------|---|
| F1 | ${BASELINE_F1} | ${CURRENT_F1} | ${DELTA_F1} |
| Accuracy | ${BASELINE_ACC} | ${CURRENT_ACC} | ${DELTA_ACC} |
EOF
```

**Корисність для review:** ревʼювер миттєво бачить, чи покращує PR метрики, без необхідності запускати код локально. Якщо Δ від'ємна — це явний сигнал для обговорення.

---

## 14. Як зробити CI швидким і стабільним, якщо повне тренування займає довго?

| Стратегія | Реалізація | Компроміс |
|---|---|---|
| **Підвибірка в CI** | `--max-rows 2000` або `CI=true` у env | Метрики можуть відрізнятися від production |
| **Кешування залежностей** | `actions/cache` для `.venv` + `enable-cache: true` у `setup-uv` | Кеш може застаріти при зміні lockfile |
| **Кешування даних** | `actions/cache` для `data/prepared/` | Дані можуть застаріти без DVC pull |
| **Розділення jobs** | `pre-train-tests` → `train` → `post-train-tests` як окремі jobs | Додають overhead на запуск VM |
| **Scheduled full run** | `cron: '0 3 * * 1'` для повного тренування | Затримка зворотного зв'язку до наступного тижня |
| **Fast CI режим** | `hydra: hpo.n_trials=3` у CI env | Smoke perевірка без повного HPO |

**Наш підхід у ЛР4:** у CI змінюємо `max_epochs` або `n_estimators` через override:
```bash
python train.py training.n_estimators=10  # замість 100
```

---

## 15. Як реалізувати policy-as-code для ML-якості у CI?

**Концепція:** правила якості версіонуються разом із кодом і виконуються автоматично — без ручних перевірок.

**Реалізація через YAML-файл якості:**
```yaml
# quality_policy.yaml
thresholds:
  f1: 0.876
  accuracy: 0.85
  max_overfitting_gap: 0.15
required_artifacts:
  - model.pkl
  - metrics.json
  - confusion_matrix.png
data_requirements:
  min_rows: 1000
  required_columns: [label]
  max_null_fraction: 0.01
```

**pytest fixture читає policy:**
```python
import yaml, json, os

@pytest.fixture(scope="session")
def policy():
    with open("quality_policy.yaml") as f:
        return yaml.safe_load(f)

def test_quality_gate_f1(policy):
    with open("metrics.json") as f:
        metrics = json.load(f)
    assert metrics["f1"] >= policy["thresholds"]["f1"]
```

**Переваги:** пороги переглядаються через PR (є явна зміна у git diff), а не приховані у secrets або hardcode. Будь-яке послаблення вимог стає видимим у code review.
