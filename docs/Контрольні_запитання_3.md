# Контрольні запитання - Відповіді (Лабораторна робота №3)

## 1. Яка різниця між параметрами моделі та гіперпараметрами?

**Параметри моделі** — навчаються автоматично під час тренування (наприклад, порогові значення розбиття у деревах Random Forest, ваги нейронної мережі). Ніколи не задаються вручну.

**Гіперпараметри** — задаються *до* навчання і контролюють сам процес навчання або структуру моделі:

| Гіперпараметр | Що контролює |
|---|---|
| `n_estimators` | Кількість дерев у лісі |
| `max_depth` | Максимальна глибина дерева |
| `min_samples_split` | Мінімум зразків для розбиття вузла |
| `C` (LogReg) | Сила регуляризації |

---

## 2. Чому Grid Search погано масштабується зі зростанням кількості гіперпараметрів?

Кількість комбінацій росте **експоненційно** — «прокляття розмірності»:

```
3 параметри × 10 значень кожен = 10³ = 1000 комбінацій
5 параметрів × 10 значень кожен = 10⁵ = 100 000 комбінацій
```

При цьому більшість комбінацій є неперспективними. На відміну від Grid Search, Bayesian Optimization (TPE) розумно обирає наступні точки на основі вже отриманих результатів, що дозволяє знаходити хороші параметри за значно меншу кількість спроб.

---

## 3. Поясніть принцип роботи TPE (Tree-structured Parzen Estimator)

TPE розділяє результати спроб на дві групи за заданим квантилем (за замовчуванням 25% кращих):

- **l(x)** — розподіл параметрів серед «хороших» trial-ів
- **g(x)** — розподіл серед «поганих» trial-ів

Наступна точка обирається там, де **l(x)/g(x)** максимальний — тобто де параметри частіше зустрічаються у хороших результатах і рідко у поганих. Це забезпечує баланс між exploitation (досліджувати відомо хороші зони) та exploration (пробувати нові).

---

## 4. Що таке Study і Trial в Optuna?

**Study** — «контейнер» всієї оптимізаційної кампанії. Зберігає всі trial-и, їх параметри, результати та стан семплера.

**Trial** — одна спроба: конкретний набір гіперпараметрів → запуск → одне значення метрики.

```python
# У нашому проекті:
study = optuna.create_study(
    direction="maximize",          # максимізуємо test_f1
    sampler=TPESampler(seed=42),   # відтворюваний TPE
)
study.optimize(objective, n_trials=20)  # 20 trials

# Результат:
# Best trial #3: test_f1=0.8823
# Params: n_estimators=227, max_depth=5, min_samples_split=20
```

---

## 5. Навіщо використовувати nested runs у MLflow при HPO?

Без nested runs 20 trial-ів і фінальний запуск виглядали б як 21 незалежний run — незрозуміло, які пов'язані між собою. Nested runs дають ієрархічну структуру:

```
hpo_study (parent run)
├── trial_0  (child run) — n_estimators=180, max_depth=12 → f1=0.86
├── trial_1  (child run) — n_estimators=67,  max_depth=28 → f1=0.83
├── trial_2  (child run) — ...
│   ...
└── trial_19 (child run) — n_estimators=227, max_depth=5  → f1=0.88 ✅
```

На parent run логуються `best_n_estimators`, `best_test_f1` — підсумок всього HPO.

---

## 6. Як гарантувати відтворюваність Optuna Study?

Фіксуємо seed у семплері:
```python
sampler = optuna.samplers.TPESampler(seed=42)
study = optuna.create_study(sampler=sampler)
```
Тоді при повторному запуску з тим самим seed і тими самими даними — послідовність параметрів у trial-ах буде ідентичною. В нашому проекті seed задається через Hydra config (`conf/config.yaml`), що робить його частиною зафіксованої конфігурації:

```yaml
optuna:
  seed: 42
```

---

## 7. Що таке pruner у Optuna і коли його варто використовувати?

**Pruner** — механізм ранньої зупинки поганих trial-ів на основі проміжних результатів. Замість того щоб чекати повного навчання для явно безперспективної спроби, pruner зупиняє її достроково.

**Коли використовувати:**
- Навчання займає багато часу (нейронні мережі, XGBoost з великою кількістю раундів)
- Є можливість отримувати проміжні метрики (по epochs чи раундам)

**Для нашого Random Forest** pruner не застосовується, оскільки RF навчається в один прохід без проміжних результатів.

Приклад з Hyperband Pruner для нейронних мереж:
```python
study = optuna.create_study(
    pruner=optuna.pruners.HyperbandPruner()
)
```

---

## 8. Інтерпретуйте результати HPO вашого проекту: компроміс якість ↔ ресурси

**Результат оптимізації:**
- **До HPO** (фіксовані параметри): `test_f1 = 0.8515`, `max_depth=15`, `n_estimators=100`
- **Після HPO** (best trial #3): `test_f1 = 0.8823`, `max_depth=5`, `n_estimators=227`

**Що це означає:**
- `max_depth=5` (менша глибина) → модель стала **менш схильна до перенавчання**: `final_train_f1=0.9668` vs `0.9974` раніше, а `test_f1` зріс з `0.8515` → `0.8823`
- `n_estimators=227` → більше дерев компенсує меншу глибину кожного, стабілізуючи передбачення

**Компроміс:**
- Час навчання фінальної моделі зріс (~2.2x більше дерев)
- Але якість test_f1 покращилась на **+3.6%** — суттєво для медичної задачі (PNEUMONIA detection)

> **Висновок:** HPO показав, що «неглибокий але широкий» ансамбль кращий за «глибокий і вузький» на цьому датасеті. Глибокі дерева перенавчалися на тренувальних даних.

---

## 9. Як коректно порівнювати TPE і Random sampler на одному датасеті?

Щоб порівняння було справедливим, потрібно:

1. **Однаковий бюджет** — однакова кількість `n_trials` для обох семплерів.
2. **Той самий seed** — передати ідентичний seed кожному семплеру, щоб результати були відтворюваними.
3. **Той самий простір пошуку** — однакові межі `suggest_int` / `suggest_float`.
4. **Та сама метрика та датасет** — виключаємо вплив випадкового split'у, використовуючи фіксований `random_state=42` у `train_test_split`.
5. **Кілька незалежних запусків** (різні seed) → порівнюємо медіану та розкид, а не лише best value.

Практично в нашому проекті:
```bash
# TPE
python src/optimize.py hpo.sampler=tpe hpo.n_trials=20
# Random
python src/optimize.py hpo.sampler=random hpo.n_trials=20
```
Порівнюємо `best_f1` та `best-so-far` криву у MLflow між двома parent runs.

---

## 10. Які кроки ви виконаєте перед переведенням моделі зі Staging у Production?

1. **Перевірка метрик** — `test_f1` нової версії ≥ поточного champion + деякий мінімальний приріст (наприклад, Δ ≥ 0.5%).
2. **Regression testing** — пройти набір фіксованих зразків (golden set) і переконатися, що передбачення не деградували на критичних випадках.
3. **Smoke test API** — підняти serving у staging середовищі і надіслати кілька тестових запитів.
4. **Перевірка артефактів** — впевнитися, що `model.pkl`, `metrics.json`, `confusion_matrix.png` збережені та прив'язані до конкретного `run_id`.
5. **Реєстрація відповідальної особи** — проставити теги `deployed_by`, `approved_by` у MLflow Model Registry.
6. **Перехід** — `client.transition_model_version_stage(name=..., stage="Production")` або оновлення аліасу `champion`.

---

## 11. Модифікуйте objective function, щоб підтримувати 5-fold cross-validation

В `objective_factory` вже реалізовано підтримку через параметр `cfg.hpo.use_cv`:

```python
if cfg.hpo.use_cv:
    X = np.concatenate([X_train, X_test], axis=0)
    y = np.concatenate([y_train, y_test], axis=0)
    score = evaluate_cv(model, X, y,
                        metric=cfg.hpo.metric,
                        seed=cfg.seed,
                        n_splits=cfg.hpo.cv_folds)  # 5
else:
    score = evaluate(model, X_train, y_train, X_test, y_test,
                     metric=cfg.hpo.metric)
```

**Вплив на стабільність:** оцінка стає більш надійною — замість одного split використовується середнє по 5 фолдах, зменшується дисперсія метрики між trial-ами.

**Вплив на час:** час одного trial зростає у ~5 разів. Для `n_trials=20` та RF це може бути прийнятним, але для нейронних мереж — критично. Компроміс: зменшити `n_trials` при включенні CV або використовувати CV лише для top-K trial-ів.

---

## 12. Опишіть ранню зупинку (early stopping) в Optuna для нейронної мережі та роль pruner

**Pruner** — механізм Optuna, що зупиняє trial достроково, якщо проміжні результати свідчать про безперспективність:

```python
study = optuna.create_study(
    direction="maximize",
    pruner=optuna.pruners.MedianPruner(n_startup_trials=5)
)

def objective(trial):
    lr = trial.suggest_float("lr", 1e-5, 1e-2, log=True)
    model = MyNet(lr=lr)
    for epoch in range(100):
        val_acc = train_one_epoch(model, ...)
        # Повідомляємо Optuna про проміжний результат
        trial.report(val_acc, step=epoch)
        # Якщо пруner вирішив зупинити — піднімаємо виняток
        if trial.should_prune():
            raise optuna.exceptions.TrialPruned()
    return val_acc
```

**MedianPruner** зупиняє trial, якщо після `k` епох його метрика нижче медіани попередніх trial-ів на тому самому кроці. **HyperbandPruner** — агресивніший: відкидає найгіршу частку на кожному bracket-і. Для Random Forest pruner не застосовується (немає проміжних результатів).

---

## 13. Як організувати HPO для кількох моделей (RF, LR, SVM, GBM, NN) з динамічним вибором через Hydra?

Використовуємо Hydra config groups (`conf/model/`) та `suggest_categorical` у Optuna:

```yaml
# conf/model/random_forest.yaml
type: random_forest

# conf/model/svm.yaml
type: svm
```

```python
def suggest_params(trial, model_type, cfg):
    # Спочатку вибираємо тип моделі
    model_type = trial.suggest_categorical(
        "model_type", ["random_forest", "logistic_regression", "svm"]
    )
    if model_type == "random_forest":
        return {"n_estimators": trial.suggest_int("n_estimators", 50, 300), ...}
    elif model_type == "logistic_regression":
        return {"C": trial.suggest_float("C", 1e-3, 1e2, log=True), ...}
    elif model_type == "svm":
        return {"C": trial.suggest_float("C", 1e-2, 1e2, log=True),
                "kernel": trial.suggest_categorical("kernel", ["rbf", "linear"]), ...}
```

Trial автоматично обирає модель та її параметри — один Study охоплює весь простір. Через Hydra можна запустити різні конфігурації порівняння:
```bash
python src/optimize.py --multirun model=random_forest,svm hpo.n_trials=20
```

---

## 14. Як інтегрувати HPO в CI/CD: коли запускати, який бюджет, що робити з артефактами?

**Коли запускати:**
- **Не в основному CI** (кожен push) — HPO займає хвилини/години.
- **По розкладу** (`schedule: cron: '0 2 * * 1'` — щопонеділка вночі) або **вручну** (`workflow_dispatch`).
- **При merge в `main`** — якщо зміни зачіпають `src/` або `config/`.

**Бюджет:**
- CI-runner: `n_trials=5–10` (smoke HPO) для перевірки коду без повного пошуку.
- Scheduled: `n_trials=30–50` з повним CV.

**Що робити з артефактами:**
```yaml
- name: Upload best model
  uses: actions/upload-artifact@v4
  with:
    name: best-model-hpo
    path: |
      models/best_model.pkl
      best_params.json
```
Або автоматично реєструвати в MLflow Registry (якщо є tracking server) та призначати стадію `Staging`.

---

## 15. Запропонуйте стратегію тегування/опису версій моделі в MLflow Registry для різних sampler-ів і конфігурацій

**Стратегія тегів на рівні Model Version:**

```python
client.set_model_version_tag(name, version, "sampler", "tpe")
client.set_model_version_tag(name, version, "model_type", "random_forest")
client.set_model_version_tag(name, version, "n_trials", "20")
client.set_model_version_tag(name, version, "seed", "42")
client.set_model_version_tag(name, version, "best_f1", "0.8823")
client.set_model_version_tag(name, version, "git_commit", os.getenv("GITHUB_SHA", "local"))
```

**Схема версіонування:**
- Версія `v1` — baseline (без HPO)
- Версія `v2` — Random sampler, 20 trials
- Версія `v3` — TPE sampler, 20 trials
- Версія `v4` — TPE sampler, 50 trials + CV

**Опис моделі** (через `client.update_model_version`):
```
TPE, n_trials=20, RF, seed=42, cv=False
Dataset: chest_xray v2.0 (dvc: abc123)
F1=0.8823, Accuracy=0.8891
```

Таким чином, через фільтр у MLflow UI можна порівнювати версії різних самплерів і конфігурацій без заглиблення в логи runs.
