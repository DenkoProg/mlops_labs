# Контрольні запитання - Відповіді (Лабораторна робота №5)

## 1. Опишіть три рівні зрілості MLOps (Google Maturity Model)

| Рівень | Назва | Ключова ознака |
|---|---|---|
| **0** | Ручний процес | Notebooks, ручна передача артефактів, немає відтворюваності |
| **1** | Автоматизація ML-пайплайну (CT) | Оркестрований пайплайн, Continuous Training за розкладом/подіями |
| **2** | Повна CI/CD + CT інтеграція | Код пайплайну проходить через CI/CD, моделі — через CT, є моніторинг drift |

**Наш проект після ЛР5** знаходиться між рівнями 1 та 2: є CT через Airflow DAG (@weekly), CI через GitHub Actions, але немає моніторингу drift у production.

---

## 2. Що таке DAG в контексті Airflow? Чому «ациклічний»?

**DAG (Directed Acyclic Graph)** — орієнтований ациклічний граф, де:
- **Вузли** — задачі (Tasks)
- **Ребра** — залежності між ними

**Ациклічний** означає: неможливо, стартувавши з будь-якого вузла, повернутися до нього, рухаючись по ребрах. Це гарантує, що пайплайн завершиться і не зависне у нескінченному циклі.

У нашому DAG (`ml_training_pipeline`):
```
prepare_data → train_model → evaluate_model → register_model
```
Якщо б `register_model` залежав від `prepare_data`, утворився б цикл — Airflow відхиляє такі DAG-и при завантаженні.

---

## 3. Навіщо multi-stage build у Docker для ML-проєктів?

**Проблема:** наївний образ із `uv`, компілятором C (для numpy/scikit), заголовками Python важить >1.5GB.

**Multi-stage вирішує це:**

```dockerfile
# Stage 1: builder — є всі build-tools
FROM python:3.11-slim AS builder
RUN pip install uv
RUN uv sync --frozen --no-dev   # компілює бінарні залежності

# Stage 2: runtime — тільки результат
FROM python:3.11-slim AS runtime
COPY --from=builder /app/.venv /app/.venv  # копіюємо готовий venv
COPY src/ ./src/
# ↑ немає uv, gcc, заголовків → образ ~400MB замість ~1.5GB
```

**Переваги для ML/CI:**
- Швидше завантаження образу в CI-runner
- Менша поверхня атаки (немає build-tools у production)
- Layer cache: якщо `pyproject.toml` не змінився — залежності не перебудовуються

---

## 4. Поясніть ключові компоненти Apache Airflow

| Компонент | Роль |
|---|---|
| **Scheduler** | Інтерпретує DAG, відстежує стан задач, ініціює запуск за розкладом |
| **Executor** | Стратегія виконання (LocalExecutor — у процесі; CeleryExecutor — розподілено) |
| **Webserver** | UI на http://localhost:8080: перегляд DAG, логів, ручний запуск |
| **Metadata DB** | PostgreSQL/SQLite: стан всіх runs, tasks, конфігурація |
| **Worker** | Процес/контейнер, що виконує код задачі |

У нашому `docker-compose.yml` використовується **LocalExecutor + PostgreSQL** — найпростіший production-ready варіант без зайвої складності Celery.

---

## 5. Що таке Continuous Training (CT) і чим воно відрізняється від CI?

**CI (Continuous Integration)** — автоматична перевірка *коду*: лінтинг, тести, DVC pipeline validation. Запускається при кожному `git push`.

**CT (Continuous Training)** — автоматичне *перенавчання моделі* за розкладом або подіями (нові дані, data drift). Запускається незалежно від змін коду.

| | CI | CT |
|---|---|---|
| **Тригер** | `git push` / `pull_request` | `@weekly` / нові дані / drift alert |
| **Артефакт** | Статус тестів, CML report | Нова версія моделі |
| **Інструмент (ЛР5)** | GitHub Actions | Airflow DAG |

**Разом** вони утворюють рівень 2 MLOps: CI перевіряє код пайплайну, CT оновлює модель.

---

## 6. Яку роль грає `register_model.py` в пайплайні?

Реалізує **якісний поріг для просування моделі** (promotion gate):

```python
current_f1 = get_current_champion_f1(client)  # з MLflow Registry

if new_f1 <= current_f1:
    log.info("Not better than champion. Skipping.")
    return

# Якщо краща — реєструємо та призначаємо аліас
mlflow.sklearn.log_model(model, registered_model_name="chest_xray_rf")
client.set_registered_model_alias("chest_xray_rf", "champion", latest.version)
```

**Чому аліас `champion`?** Аліаси стабільніші за номери версій — клієнти inference завжди завантажують `champion`, не прив'язуючись до `v5`, `v6` і т.д.

---

## 7. Чому тести DAG-ів важливо запускати у CI без живого Airflow?

DAG-тести перевіряють **структуру** графа, а не виконання задач. Запуск повного Airflow у CI вимагає PostgreSQL, Scheduler, Webserver — це дорого (час + ресурси).

Наш підхід у `tests/test_dag.py`:
```python
# Імпортуємо DAG-модуль безпосередньо, без Airflow runtime
spec = importlib.util.spec_from_file_location("ml_pipeline_dag", DAG_PATH)
module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(module)
dag = module.dag
```

Це дає **швидкий зворотний зв'язок** (1 сек vs. 2+ хв) при:
- Синтаксичні помилки в DAG-файлі
- Неправильний порядок задач або відсутня залежність
- Неправильний `dag_id` або `schedule`

---

## 8. Як Airflow забезпечує відтворюваність CT-запусків?

1. **`catchup=False`** — не виконує пропущені запуски за минулі тижні (уникаємо лавини runs)
2. **`start_date` фіксований** — детермінований розклад
3. **Retries + retry_delay** — автоматичне повторення при збоях (мережа, DVC pull)
4. **Логи кожного run** зберігаються в Airflow Metadata DB — повна аудит-стежка
5. **DVC lock у Git** — `dvc repro` у `train_model` tasks завжди тренує на тій версії даних, що зафіксована в `dvc.lock` поточного коміту
6. **MLflow run_id** — кожна реєстрація пов'язана з конкретним `run_id`, що Contains commit hash, params, metrics
