# Контрольні запитання - Відповіді

## 1. У чому полягає фундаментальна відмінність між артефактами коду (у DevOps) та артефактами ML-моделей?

**Код (DevOps):** детермінований, однакові входи → однакові виходи.

**ML-модель:** недетермінована, залежить від даних, random_state, версій бібліотек. Потребує логування даних, гіперпараметрів і метрик для відтворення.

---

## 2. Чому папку mlruns додають у .gitignore при локальній розробці?

- Містить локальні експерименти кожного розробника
- Постійно змінюється та росте в розмірі
- Викликає конфлікти при merge
- Для команди потрібен централізований MLflow tracking server

---

## 3. Які переваги дає використання віртуальних середовищ у командній розробці?

- **Ізоляція залежностей** між проектами
- **Відтворюваність** через фіксовані версії (pyproject.toml, uv.lock)
- **Уникнення конфліктів** версій бібліотек
- **Чисте середовище** без глобальних пакетів

---

## 4. Три основні компоненти MLflow Tracking з прикладами

1. **Parameters (гіперпараметри):**
   ```python
   mlflow.log_param("max_depth", 50)
   mlflow.log_param("n_estimators", 100)
   ```

2. **Metrics (метрики):**
   ```python
   mlflow.log_metric("test_accuracy", 0.7788)
   mlflow.log_metric("train_f1", 1.0000)
   ```

3. **Artifacts (файли):**
   ```python
   mlflow.log_artifact("confusion_matrix.png")
   mlflow.sklearn.log_model(model, "random_forest_model")
   ```

---

## 5. Як забезпечити повну відтворюваність, якщо дані постійно оновлюються?

**Інструменти:**
- **DVC (Data Version Control)** - версіонування даних
- **MLflow** - логування dataset_version як тег
- **Checksums/hashes** - перевірка цілісності даних
- **Data snapshots** - збереження конкретних версій датасету
- **Фіксація random_state** для розбиття train/test

**Приклад:**
```python
mlflow.set_tag("dataset_version", "chest_xray_v1")
mlflow.set_tag("data_hash", "sha256:abc123...")
```

---

## 6. Різниця між mlflow.log_artifact та mlflow.log_model

**`log_artifact`:**
- Логує довільний файл (PNG, CSV, JSON)
- Не містить метаданих про модель

**`log_model`:**
- Зберігає модель зі структурою MLmodel
- Додає: signature (типи входів/виходів), conda/pip залежності, flavors
- Дозволяє завантажити модель через `mlflow.sklearn.load_model()`
- Придатна для production deployment

---

## 7. Як MLflow дозволяє порівнювати моделі з Cross-Validation?

**Підходи:**
1. **Окремі runs для кожного fold:**
   ```python
   for fold in range(5):
       with mlflow.start_run(run_name=f"fold_{fold}"):
           mlflow.log_metric("cv_accuracy", acc)
   ```

2. **Логування середніх метрик:**
   ```python
   mlflow.log_metric("cv_mean_accuracy", np.mean(scores))
   mlflow.log_metric("cv_std_accuracy", np.std(scores))
   ```

3. **Nested runs (Parent-Child):**
   ```python
   with mlflow.start_run(run_name="cv_experiment"):
       for fold in folds:
           with mlflow.start_run(nested=True):
               # логування fold
   ```

---

## 8. Архітектура для команди з 5 Data Scientists

**Налаштування:**
```bash
# Centralізований MLflow Tracking Server
mlflow server \
  --backend-store-uri postgresql://db_host/mlflow \
  --default-artifact-root s3://mlflow-artifacts/
```

**Клієнти підключаються:**
```python
mlflow.set_tracking_uri("http://mlflow-server:5000")
```

**Де зберігаються:**
- **Метрики/параметри:** PostgreSQL/MySQL (backend-store-uri)
- **Артефакти/моделі:** S3/Azure Blob/HDFS (artifact-root)

---

## 9. Чому результати можуть відрізнятися на 4-му знаку після коми?

**Причини:**
1. **Версії бібліотек:**
   - scikit-learn 1.3.0 vs 1.4.0 можуть мати різні алгоритми
   - numpy зміни в random generators

2. **Floating point arithmetic:**
   - Різні CPU (Intel vs ARM) → різні округлення
   - Порядок операцій впливає на точність
   - Parallel processing (n_jobs=-1) → недетермінований порядок

3. **System dependencies:**
   - BLAS/LAPACK версії
   - Compiler optimization flags

**Рішення:** фіксувати versions у pyproject.toml, uv.lock

---

## 10. Чому логування тільки test метрик недостатнє?

**Проблеми:**
- Не видно **overfitting** (train vs test gap)
- Немає **validation set** для hyperparameter tuning
- Test set використовується як validation → **data leakage**

**Правильний підхід:**

```
Train set (70%) → навчання моделі
Validation set (15%) → hyperparameter tuning, early stopping
Test set (15%) → фінальна оцінка (використовується 1 раз!)
```

**У MLflow:**
```python
mlflow.log_metric("train_accuracy", 1.0000)  # Виявляє overfitting
mlflow.log_metric("val_accuracy", 0.85)      # Для tuning
mlflow.log_metric("test_accuracy", 0.7788)   # Final score
```

**Ваш проект показав overfitting:**
- `max_depth=50`: train_accuracy=1.0, test_accuracy=0.7788 → переучення!
- Без train метрик ви б цього не побачили.
